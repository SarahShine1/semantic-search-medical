"""
Pr√©traitement des donn√©es MedQuAD
- Nettoyage des textes
- Combinaison Question + Answer
- Pr√©paration pour les embeddings
"""

import pandas as pd
import re
import os
import sys


sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import Config

def clean_text(text):
    """
    Nettoie un texte m√©dical
    
    Args:
        text (str): Texte brut
        
    Returns:
        str: Texte nettoy√©
    """
    if pd.isna(text):
        return ""
    
    # Convertir en string
    text = str(text)
    
    # Remplacer les sauts de ligne multiples par un seul espace
    text = re.sub(r'\n+', ' ', text)
    
    # Remplacer les espaces multiples par un seul
    text = re.sub(r'\s+', ' ', text)
    
    # Enlever les espaces au d√©but et √† la fin
    text = text.strip()
    
    return text

def combine_question_answer(row):
    """
    Combine Question + Answer pour avoir plus de contexte
    
    Args:
        row: Ligne du DataFrame
        
    Returns:
        str: Texte combin√©
    """
    question = clean_text(row['Question'])
    answer = clean_text(row['Answer'])
    
    # Format: "Question: ... Answer: ..."
    combined = f"Question: {question} Answer: {answer}"
    
    return combined

def preprocess_dataset(input_path, output_path, sample_size=None):
    """
    Pr√©traite le dataset MedQuAD complet
    
    Args:
        input_path (str): Chemin du CSV brut
        output_path (str): Chemin du CSV trait√©
        sample_size (int, optional): Prendre seulement N documents (pour test)
    
    Returns:
        pd.DataFrame: Dataset pr√©trait√©
    """
    
    print("=" * 60)
    print("üßπ PR√âTRAITEMENT DES DONN√âES MEDQUAD")
    print("=" * 60)
    
    # 1. Charger les donn√©es
    print(f"\n1Ô∏è‚É£ Chargement du dataset: {input_path}")
    df = pd.read_csv(input_path)
    print(f"   ‚úÖ {len(df)} documents charg√©s")
    print(f"   üìã Colonnes: {list(df.columns)}")
    
    # 2. √âchantillonnage (optionnel - pour tester rapidement)
    if sample_size and sample_size < len(df):
        print(f"\nüìä √âchantillonnage: {sample_size} documents (pour test rapide)")
        df = df.sample(n=sample_size, random_state=42).reset_index(drop=True)
        print(f"   ‚úÖ {len(df)} documents s√©lectionn√©s")
    
    # 3. V√©rifier les valeurs manquantes
    print("\n2Ô∏è‚É£ V√©rification des valeurs manquantes...")
    missing = df[['Question', 'Answer']].isna().sum()
    print(f"   - Questions manquantes: {missing['Question']}")
    print(f"   - R√©ponses manquantes: {missing['Answer']}")
    
    # Supprimer les lignes avec Question OU Answer vide
    initial_len = len(df)
    df = df.dropna(subset=['Question', 'Answer'])
    removed = initial_len - len(df)
    if removed > 0:
        print(f"   üóëÔ∏è {removed} lignes supprim√©es (donn√©es manquantes)")
    else:
        print(f"   ‚úÖ Aucune donn√©e manquante")
    
    # 4. Nettoyage des textes
    print("\n3Ô∏è‚É£ Nettoyage des textes...")
    df['question_clean'] = df['Question'].apply(clean_text)
    df['answer_clean'] = df['Answer'].apply(clean_text)
    print("   ‚úÖ Textes nettoy√©s")
    
    # V√©rifier la longueur des textes
    df['question_length'] = df['question_clean'].str.len()
    df['answer_length'] = df['answer_clean'].str.len()
    
    print(f"\n   üìè Statistiques de longueur:")
    print(f"      Questions:")
    print(f"         - Moyenne: {df['question_length'].mean():.0f} caract√®res")
    print(f"         - Min: {df['question_length'].min()}")
    print(f"         - Max: {df['question_length'].max()}")
    print(f"      R√©ponses:")
    print(f"         - Moyenne: {df['answer_length'].mean():.0f} caract√®res")
    print(f"         - Min: {df['answer_length'].min()}")
    print(f"         - Max: {df['answer_length'].max()}")
    
    # 5. Combiner Question + Answer
    print("\n4Ô∏è‚É£ Combinaison Question + Answer...")
    df['combined_text'] = df.apply(combine_question_answer, axis=1)
    df['combined_length'] = df['combined_text'].str.len()
    print("   ‚úÖ Textes combin√©s")
    print(f"      - Longueur moyenne: {df['combined_length'].mean():.0f} caract√®res")
    
    # 6. Cr√©er le DataFrame final
    print("\n5Ô∏è‚É£ Cr√©ation du dataset final...")
    df_final = pd.DataFrame({
        'id': range(len(df)),
        'question': df['question_clean'],
        'answer': df['answer_clean'],
        'combined_text': df['combined_text'],
        'category': df['qtype'],
        'source': 'MedQuAD'
    })
    
    print(f"   ‚úÖ {len(df_final)} documents pr√™ts")
    
    # 7. Aper√ßu des donn√©es
    print("\n6Ô∏è‚É£ Aper√ßu des donn√©es pr√©trait√©es:")
    print("\n" + "=" * 60)
    for i in range(min(2, len(df_final))):
        row = df_final.iloc[i]
        print(f"\nüìÑ Document {i+1}:")
        print(f"   Cat√©gorie: {row['category']}")
        print(f"   Question: {row['question'][:100]}...")
        print(f"   Answer: {row['answer'][:100]}...")
        print(f"   Combined: {row['combined_text'][:150]}...")
    print("\n" + "=" * 60)
    
    # 8. Sauvegarder
    print(f"\n7Ô∏è‚É£ Sauvegarde du dataset pr√©trait√©...")
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    df_final.to_csv(output_path, index=False)
    print(f"   ‚úÖ Sauvegard√©: {output_path}")
    
    # 9. Statistiques finales
    print("\n" + "=" * 60)
    print("‚úÖ PR√âTRAITEMENT TERMIN√â!")
    print("=" * 60)
    print(f"\nüìä R√©sum√©:")
    print(f"   - Documents originaux: {initial_len}")
    print(f"   - Documents apr√®s nettoyage: {len(df_final)}")
    print(f"   - Cat√©gories uniques: {df_final['category'].nunique()}")
    print(f"   - Fichier de sortie: {output_path}")
    print(f"   - Pr√™t pour la g√©n√©ration d'embeddings!")
    
    # Distribution par cat√©gorie
    print(f"\nüìà Distribution par cat√©gorie:")
    print(df_final['category'].value_counts().head(10))
    
    return df_final

if __name__ == "__main__":
    # Chemins
    input_csv = os.path.join(Config.RAW_DATA_DIR, 'medquad_raw.csv')
    output_csv = os.path.join(Config.PROCESSED_DATA_DIR, 'medquad_processed.csv')
    
    print(f"\nüìÇ Chemins:")
    print(f"   Input:  {input_csv}")
    print(f"   Output: {output_csv}")
    
    # Pour TEST RAPIDE: utilise sample_size=1000
    # Pour PRODUCTION: sample_size=None (tous les documents)
    
    # CHOISIS TON MODE:
    MODE = "PRODUCTION"  # ou "PRODUCTION"
    
    if MODE == "TEST":
        print("\n‚ö° MODE TEST: 1000 documents seulement")
        df = preprocess_dataset(input_csv, output_csv, sample_size=1000)
    else:
        print("\nüè≠ MODE PRODUCTION: Tous les documents")
        df = preprocess_dataset(input_csv, output_csv, sample_size=None)
    
    print("\n‚ú® Fichier pr√™t pour l'√©tape suivante: G√©n√©ration d'embeddings")
